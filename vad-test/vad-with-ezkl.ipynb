{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet\n",
    "\n",
    "https://huggingface.co/datasets/ProgramComputer/voxceleb\n",
    "\n",
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/python/3.12.1/lib/python3.12/site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -Uq datasets transformers transformers[onnx] ezkl tf-keras librosa \"soundfile>=0.12.1\" torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reading metadata...: 948736it [00:18, 51827.88it/s]\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'THE', 'start_time': np.float64(0.7), 'end_time': np.float64(0.78)},\n",
       " {'word': 'TRICK',\n",
       "  'start_time': np.float64(0.88),\n",
       "  'end_time': np.float64(1.08)},\n",
       " {'word': 'APPEARS',\n",
       "  'start_time': np.float64(1.2),\n",
       "  'end_time': np.float64(1.64)}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how to retrieve time steps for a model\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "\n",
    "# import model, feature extractor, tokenizer\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# load first sample of English common_voice\n",
    "dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"train\", streaming=True, trust_remote_code=True)\n",
    "dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n",
    "dataset_iter = iter(dataset)\n",
    "sample = next(dataset_iter)\n",
    "\n",
    "# forward sample through model to get greedily predicted transcription ids\n",
    "input_values = feature_extractor(sample[\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    "logits = model(input_values).logits[0]\n",
    "pred_ids = torch.argmax(logits, axis=-1)\n",
    "\n",
    "# retrieve word stamps (analogous commands for `output_char_offsets`)\n",
    "outputs = tokenizer.decode(pred_ids, output_word_offsets=True)\n",
    "# compute `time_offset` in seconds as product of downsampling ratio and sampling_rate\n",
    "time_offset = model.config.inputs_to_logits_ratio / feature_extractor.sampling_rate\n",
    "\n",
    "word_offsets = [\n",
    "    {\n",
    "        \"word\": d[\"word\"],\n",
    "        \"start_time\": round(d[\"start_offset\"] * time_offset, 2),\n",
    "        \"end_time\": round(d[\"end_offset\"] * time_offset, 2),\n",
    "    }\n",
    "    for d in outputs.word_offsets\n",
    "]\n",
    "# compare word offsets with audio `en_train_0/common_voice_en_19121553.mp3` online on the dataset viewer:\n",
    "# https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/en\n",
    "word_offsets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX Version of Wav2Vec2\n",
    "\n",
    "https://huggingface.co/optimum/wav2vec2-base-superb-sv\n",
    "https://huggingface.co/optimum/wav2vec2-base-superb-sd\n",
    "\n",
    "To manually convert model to ONNX\n",
    "\n",
    "https://huggingface.co/docs/transformers/serialization#exporting-a-model-with-transformersonnx\n",
    "\n",
    "Use transformers.onnx package as a Python module to export a checkpoint using a ready-made configuration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m transformers.onnx --model=facebook/wav2vec2-base-960h onnx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-02 03:55:02.647101: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-02 03:55:02.729125: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-02 03:55:02.811940: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730519702.983406   23486 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730519703.058248   23486 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-02 03:55:03.562571: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-02 03:55:15.935168: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-02 03:55:16.128840: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-02 03:55:16.250835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730519716.449181   23569 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730519716.494576   23569 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-02 03:55:17.164566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EZKL \n",
    "\n",
    "use the EZKL package to run a publicly known / committed to network on some private data, producing a public output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if notebook is in colab\n",
    "try:\n",
    "    # install ezkl\n",
    "    import google.colab\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ezkl\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"onnx\"])\n",
    "\n",
    "# rely on local installation of ezkl if the notebook is not in colab\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# here we create and (potentially train a model)\n",
    "\n",
    "# make sure you have the dependencies required here already installed\n",
    "from torch import nn\n",
    "import ezkl\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "# Defines the model\n",
    "# we got convs, we got relu, we got linear layers\n",
    "# What else could one want ????\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=5, stride=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.d1 = nn.Linear(48, 48)\n",
    "        self.d2 = nn.Linear(48, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 32x1x28x28 => 32x32x26x26\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # flatten => 32 x (32*26*26)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "\n",
    "        # 32 x (32*26*26) => 32x128\n",
    "        x = self.d1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # logits => 32x10\n",
    "        logits = self.d2(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "circuit = MyModel()\n",
    "\n",
    "# Train the model as you like here (skipped for brevity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (conv1): Conv2d(1, 2, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv2): Conv2d(2, 3, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (relu): ReLU()\n",
       "  (d1): Linear(in_features=48, out_features=48, bias=True)\n",
       "  (d2): Linear(in_features=48, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('network.onnx')\n",
    "compiled_model_path = os.path.join('network.compiled')\n",
    "pk_path = os.path.join('test.pk')\n",
    "vk_path = os.path.join('test.vk')\n",
    "settings_path = os.path.join('settings.json')\n",
    "\n",
    "witness_path = os.path.join('witness.json')\n",
    "data_path = os.path.join('input.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shape = [1, 28, 28]\n",
    "# After training, export to onnx (network.onnx) and create a data file (input.json)\n",
    "x = 0.1*torch.rand(1,*shape, requires_grad=True)\n",
    "\n",
    "# Flips the neural net into inference mode\n",
    "circuit.eval()\n",
    "\n",
    "    # Export the model\n",
    "torch.onnx.export(circuit,               # model being run\n",
    "                      x,                   # model input (or a tuple for multiple inputs)\n",
    "                      model_path,            # where to save the model (can be a file or file-like object)\n",
    "                      export_params=True,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=10,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names = ['input'],   # the model's input names\n",
    "                      output_names = ['output'], # the model's output names\n",
    "                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                    'output' : {0 : 'batch_size'}})\n",
    "\n",
    "data_array = ((x).detach().numpy()).reshape([-1]).tolist()\n",
    "\n",
    "data = dict(input_data = [data_array])\n",
    "\n",
    "    # Serialize data into file:\n",
    "json.dump( data, open(data_path, 'w' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_run_args = ezkl.PyRunArgs()\n",
    "py_run_args.input_visibility = \"private\"\n",
    "py_run_args.output_visibility = \"public\"\n",
    "py_run_args.param_visibility = \"fixed\" # private by default\n",
    "\n",
    "res = ezkl.gen_settings(model_path, settings_path, py_run_args=py_run_args)\n",
    "\n",
    "assert res == True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[tensor] decomposition error: integer -545140602 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -545140602 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -2182662032 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -2182662032 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -8716885742 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -8716885742 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -4363889902 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -4363889902 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "Using 2 columns for non-linearity table.\n",
      "Using 2 columns for non-linearity table.\n",
      "Using 4 columns for non-linearity table.\n",
      "Using 4 columns for non-linearity table.\n",
      "Using 4 columns for non-linearity table.\n",
      "Using 4 columns for non-linearity table.\n",
      "[tensor] decomposition error: integer -17428015476 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -17428015476 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "Using 4 columns for non-linearity table.\n",
      "Using 7 columns for non-linearity table.\n",
      "[tensor] decomposition error: integer -34863632895 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -34863632895 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "\n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 13, param_scale: 13, scale_input_multiplier: 2) ------------->\n",
      "\n",
      "+-----------------+----------------+----------------+-----------------+----------------+------------------+----------------+------------------+---------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error   | max_error      | min_error       | mean_abs_error | median_abs_error | max_abs_error  | min_abs_error    | mean_squared_error  | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+----------------+----------------+-----------------+----------------+------------------+----------------+------------------+---------------------+--------------------+------------------------+\n",
      "| 0.0000004942808 | 0.000010639429 | 0.000029850751 | -0.000050500035 | 0.000011237683 | 0.000010639429   | 0.000050500035 | 0.00000008940697 | 0.00000000021887063 | -0.000044344313    | 0.00026125702          |\n",
      "+-----------------+----------------+----------------+-----------------+----------------+------------------+----------------+------------------+---------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_path = os.path.join(\"calibration.json\")\n",
    "\n",
    "data_array = (torch.rand(20, *shape, requires_grad=True).detach().numpy()).reshape([-1]).tolist()\n",
    "\n",
    "data = dict(input_data = [data_array])\n",
    "\n",
    "# Serialize data into file:\n",
    "json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "\n",
    "await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# srs path\n",
    "res = await ezkl.get_srs( settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now generate the witness file\n",
    "\n",
    "res = await ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "assert os.path.isfile(witness_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HERE WE SETUP THE CIRCUIT PARAMS\n",
    "# WE GOT KEYS\n",
    "# WE GOT CIRCUIT PARAMETERS\n",
    "# EVERYTHING ANYONE HAS EVER NEEDED FOR ZK\n",
    "\n",
    "\n",
    "\n",
    "res = ezkl.setup(\n",
    "        compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "\n",
    "    )\n",
    "\n",
    "assert res == True\n",
    "assert os.path.isfile(vk_path)\n",
    "assert os.path.isfile(pk_path)\n",
    "assert os.path.isfile(settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': [['1ce0480000000000000000000000000000000000000000000000000000000000', '1a66f1ef93f5e1439170b97948e833285d588181b64550b829a031e1724e6430', '3d6fa0ef93f5e1439170b97948e833285d588181b64550b829a031e1724e6430', '937af3ef93f5e1439170b97948e833285d588181b64550b829a031e1724e6430', 'f88e580000000000000000000000000000000000000000000000000000000000', '4feb7eef93f5e1439170b97948e833285d588181b64550b829a031e1724e6430', '9db0e4ef93f5e1439170b97948e833285d588181b64550b829a031e1724e6430', '0328630000000000000000000000000000000000000000000000000000000000', 'b6dbdcef93f5e1439170b97948e833285d588181b64550b829a031e1724e6430', 'bb36960000000000000000000000000000000000000000000000000000000000']], 'proof': '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', 'transcript_type': 'EVM'}\n"
     ]
    }
   ],
   "source": [
    "# GENERATE A PROOF\n",
    "\n",
    "\n",
    "proof_path = os.path.join('test.pf')\n",
    "\n",
    "res = ezkl.prove(\n",
    "        witness_path,\n",
    "        compiled_model_path,\n",
    "        pk_path,\n",
    "        proof_path,\n",
    "\n",
    "        \"single\",\n",
    "    )\n",
    "\n",
    "print(res)\n",
    "assert os.path.isfile(proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified\n"
     ]
    }
   ],
   "source": [
    "# VERIFY IT\n",
    "\n",
    "res = ezkl.verify(\n",
    "        proof_path,\n",
    "        settings_path,\n",
    "        vk_path,\n",
    "\n",
    "    )\n",
    "\n",
    "assert res == True\n",
    "print(\"verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EZKL \n",
    "https://docs.ezkl.xyz/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 : wav2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-02 07:44:34.628520: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730533474.643587  107667 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730533474.648178  107667 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-02 07:44:34.665831: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m processor \u001b[38;5;241m=\u001b[39m Wav2Vec2Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-base-960h\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Export to ONNX\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwav2vec.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/__init__.py:375\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, report, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining, **_)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dynamic_shapes:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe exporter only supports dynamic shapes \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthrough parameter dynamic_axes when dynamo=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m     )\n\u001b[0;32m--> 375\u001b[0m \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/utils.py:502\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;241m+\u001b[39m (kwargs,)\n\u001b[0;32m--> 502\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/utils.py:1564\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1562\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1564\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1578\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1579\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1580\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/utils.py:1113\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m   1112\u001b[0m model \u001b[38;5;241m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1113\u001b[0m graph, params, torch_out, module \u001b[38;5;241m=\u001b[39m \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/utils.py:997\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    992\u001b[0m     graph \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m    993\u001b[0m         graph, flattened_args, param_count_list, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    994\u001b[0m     )\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, params, torch_out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 997\u001b[0m graph, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_and_get_graph_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    999\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/utils.py:904\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    902\u001b[0m prev_autocast_cache_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    903\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 904\u001b[0m trace_graph, torch_out, inputs_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    913\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/jit/_trace.py:1500\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1499\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m-> 1500\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mONNXTracedModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/jit/_trace.py:139\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 139\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/jit/_trace.py:130\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    129\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 130\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    132\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1726\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1724\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1726\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2229\u001b[0m, in \u001b[0;36mWav2Vec2ForCTC.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   2226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size:\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel values must be <= vocab_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2229\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwav2vec2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2232\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2237\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2238\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1726\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1724\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1726\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1810\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1805\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1806\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1807\u001b[0m )\n\u001b[1;32m   1808\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1810\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1811\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m extract_features\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;66;03m# compute reduced attention_mask corresponding to feature vectors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1726\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1724\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1726\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:451\u001b[0m, in \u001b[0;36mWav2Vec2FeatureEncoder.forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_values):\n\u001b[0;32m--> 451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43minput_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# make sure hidden_states require grad for gradient_checkpointing\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_grad \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'tuple'"
     ]
    }
   ],
   "source": [
    "# Using Wav2Vec with EZKL\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(model, \"wav2vec.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.wav\n",
      "samplerate: 44100 Hz\n",
      "channels: 2\n",
      "duration: 2.879 s\n",
      "format: WAV (Microsoft) [WAV]\n",
      "subtype: Signed 16 bit PCM [PCM_16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to generate settings: [graph] [tract] Undetermined symbol in expression: sequence_length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(audio_info)\n\u001b[1;32m    117\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Wav2Vec2EZKL()\n\u001b[0;32m--> 118\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m, in \u001b[0;36mWav2Vec2EZKL.run_workflow\u001b[0;34m(self, audio_path)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_workflow\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio_path):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     proof_generated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proof(audio_path)\n\u001b[1;32m    105\u001b[0m     verification_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_proof()\n",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m, in \u001b[0;36mWav2Vec2EZKL.setup_circuit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_circuit\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Generate settings\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 53\u001b[0m         \u001b[43mezkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;66;03m# Calibrate settings\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         ezkl\u001b[38;5;241m.\u001b[39mcalibrate_settings(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings_path, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresources\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to generate settings: [graph] [tract] Undetermined symbol in expression: sequence_length"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torchaudio\n",
    "import ezkl\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "class Wav2Vec2EZKL:\n",
    "    def __init__(self, model_name=\"facebook/wav2vec2-base\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "        self.model = Wav2Vec2Model.from_pretrained(model_name).to(self.device)\n",
    "        self.sample_rate = 16000\n",
    "        self.window_size = 16000\n",
    "        \n",
    "        # EZKL paths\n",
    "        self.model_path = Path(\"wav2vec2.onnx\")\n",
    "        self.settings_path = Path(\"settings.json\")\n",
    "        self.compiled_path = Path(\"model.ezkl\")\n",
    "        self.pk_path = Path(\"proving.key\")\n",
    "        self.vk_path = Path(\"verification.key\")\n",
    "        self.proof_path = Path(\"proof.json\")\n",
    "        self.input_path = Path(\"input.json\")\n",
    "        \n",
    "        # Export to ONNX if not exists\n",
    "        if not self.model_path.exists():\n",
    "            self.export_to_onnx()\n",
    "\n",
    "    def export_to_onnx(self):\n",
    "        dummy_input = torch.randn(1, self.window_size)\n",
    "        input_names = [\"input_values\"]\n",
    "        output_names = [\"last_hidden_state\"]\n",
    "        \n",
    "        torch.onnx.export(\n",
    "            self.model,\n",
    "            dummy_input,\n",
    "            self.model_path,\n",
    "            input_names=input_names,\n",
    "            output_names=output_names,\n",
    "            dynamic_axes={'input_values': {0: 'batch_size'}}\n",
    "        )\n",
    "\n",
    "    def preprocess_audio(self, audio_path):\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        return waveform.squeeze()\n",
    "    \n",
    "    def setup_circuit(self):\n",
    "        # Generate settings\n",
    "        if not self.settings_path.exists():\n",
    "            ezkl.gen_settings(self.model_path, self.settings_path)\n",
    "\n",
    "            # Calibrate settings\n",
    "            ezkl.calibrate_settings(self.model_path, self.settings_path, target=\"resources\")\n",
    "\n",
    "        if not self.compiled_path.exists():\n",
    "            # Compile circuit\n",
    "            ezkl.compile_circuit(\n",
    "                self.model_path, \n",
    "                self.compiled_path, \n",
    "                self.settings_path\n",
    "            )\n",
    "        \n",
    "        # Generate proving/verification keys\n",
    "        if not self.pk_path.exists() or not self.vk_path.exists():\n",
    "            ezkl.setup(\n",
    "                self.compiled_path,\n",
    "                self.pk_path,\n",
    "                self.vk_path,\n",
    "                self.settings_path\n",
    "            )\n",
    "\n",
    "    def create_proof(self, audio_path):\n",
    "        # Preprocess audio\n",
    "        waveform = self.preprocess_audio(audio_path)\n",
    "        inputs = self.processor(waveform, sampling_rate=self.sample_rate, return_tensors=\"pt\")\n",
    "        input_values = inputs.input_values.numpy().tolist()\n",
    "        \n",
    "        # Create input file\n",
    "        ezkl.create_input_file(input_values, self.input_path)\n",
    "        \n",
    "        # Generate proof\n",
    "        ezkl.prove(\n",
    "            self.model_path,\n",
    "            self.pk_path,\n",
    "            self.proof_path,\n",
    "            self.settings_path,\n",
    "            self.input_path\n",
    "        )\n",
    "        \n",
    "        return self.proof_path.exists()\n",
    "\n",
    "    def verify_proof(self):\n",
    "        return ezkl.verify(\n",
    "            self.proof_path,\n",
    "            self.vk_path,\n",
    "            self.settings_path\n",
    "        )\n",
    "\n",
    "    def run_workflow(self, audio_path):\n",
    "        self.setup_circuit()\n",
    "        proof_generated = self.create_proof(audio_path)\n",
    "        verification_status = self.verify_proof()\n",
    "        \n",
    "        return {\n",
    "            \"proof_generated\": proof_generated,\n",
    "            \"verification_status\": verification_status\n",
    "        }\n",
    "\n",
    "# Usage:\n",
    "import soundfile as sf\n",
    "audio_info = sf.info(\"test.wav\")\n",
    "print(audio_info)\n",
    "\n",
    "pipeline = Wav2Vec2EZKL()\n",
    "results = pipeline.run_workflow(\"test.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 : Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-02 11:51:12.248109: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730548272.443734   68596 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730548272.551724   68596 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-02 11:51:13.203345: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.wav\n",
      "samplerate: 44100 Hz\n",
      "channels: 2\n",
      "duration: 2.879 s\n",
      "format: WAV (Microsoft) [WAV]\n",
      "subtype: Signed 16 bit PCM [PCM_16]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import torchaudio\n",
    "import ezkl\n",
    "from pathlib import Path\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "class WhisperEZKL:\n",
    "    def __init__(self, model_name=\"openai/whisper-tiny\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_name)\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.sampling_rate = 16000\n",
    "        \n",
    "        # EZKL paths\n",
    "        self.model_path = Path(\"whisper.onnx\")\n",
    "        self.settings_path = Path(\"settings.json\")\n",
    "        self.compiled_path = Path(\"circuit.ezkl\")\n",
    "        self.pk_path = Path(\"proving.key\")\n",
    "        self.vk_path = Path(\"verification.key\")\n",
    "        self.proof_path = Path(\"proof.json\")\n",
    "        self.input_path = Path(\"input.json\")\n",
    "        \n",
    "        # Export to ONNX if not exists\n",
    "        if not self.model_path.exists():\n",
    "            self.export_to_onnx()\n",
    "\n",
    "    def export_to_onnx(self):\n",
    "        dummy_input = torch.randn(1, 80, 3000)\n",
    "        dummy_tokens = torch.zeros(1, 448, dtype=torch.long) \n",
    "\n",
    "        input_names = [\"input_values\"]\n",
    "        output_names = [\"last_hidden_state\"]\n",
    "        \n",
    "        torch.onnx.export(\n",
    "            self.model,\n",
    "            (dummy_input, dummy_tokens),\n",
    "            self.model_path,\n",
    "            input_names=[\"input_features\", \"decoder_input_ids\"],\n",
    "            output_names=[\"logits\"],\n",
    "            dynamic_axes={\n",
    "                \"input_features\": {0: \"batch\", 2: \"sequence\"},\n",
    "                \"decoder_input_ids\": {0: \"batch\", 1: \"sequence\"}\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def preprocess_audio(self, audio_path):\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        return waveform.squeeze()\n",
    "\n",
    "    def setup_circuit(self):\n",
    "        # Generate settings\n",
    "        if not self.settings_path.exists():\n",
    "            ezkl.gen_settings(self.model_path, self.settings_path)\n",
    "\n",
    "            # Calibrate settings\n",
    "            ezkl.calibrate_settings(self.model_path, self.settings_path, target=\"resources\")\n",
    "\n",
    "        if not self.compiled_path.exists():\n",
    "            # Compile circuit\n",
    "            ezkl.compile_circuit(self.model_path, self.compiled_path, self.settings_path)\n",
    "        \n",
    "        # Generate proving/verification keys\n",
    "        if not self.pk_path.exists() or not self.vk_path.exists():\n",
    "            ezkl.setup(\n",
    "                self.compiled_path,\n",
    "                self.pk_path,\n",
    "                self.vk_path,\n",
    "                self.settings_path\n",
    "            )\n",
    "\n",
    "    def create_proof(self, audio_path):\n",
    "        # Preprocess audio\n",
    "        waveform = self.preprocess_audio(audio_path)\n",
    "        inputs = self.processor(waveform, sampling_rate=self.sample_rate, return_tensors=\"pt\")\n",
    "        input_values = inputs.input_values.numpy().tolist()\n",
    "        \n",
    "        # Create input file\n",
    "        ezkl.create_input_file(input_values, self.input_path)\n",
    "        \n",
    "        # Generate proof\n",
    "        ezkl.prove(\n",
    "            self.model_path,\n",
    "            self.pk_path,\n",
    "            self.proof_path,\n",
    "            self.settings_path,\n",
    "            self.input_path\n",
    "        )\n",
    "        \n",
    "        return self.proof_path.exists()\n",
    "\n",
    "    def verify_proof(self):\n",
    "        return ezkl.verify(\n",
    "            self.proof_path,\n",
    "            self.vk_path,\n",
    "            self.settings_path\n",
    "        )\n",
    "\n",
    "    def run_workflow(self, audio_path):\n",
    "        try:\n",
    "            self.setup_circuit()\n",
    "            proof_generated = self.create_proof(audio_path)\n",
    "            verification_status = self.verify_proof()\n",
    "            \n",
    "            return {\n",
    "                \"proof_generated\": proof_generated,\n",
    "                \"verification_status\": verification_status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "# Usage:\n",
    "import soundfile as sf\n",
    "audio_info = sf.info(\"test.wav\")\n",
    "print(audio_info)\n",
    "\n",
    "pipeline = WhisperEZKL()\n",
    "results = pipeline.run_workflow(\"test.wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 : Silero VAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import ezkl\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "class SileroEZKL:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.sample_rate = 16000\n",
    "        self.window_size = 1536\n",
    "        \n",
    "        # EZKL paths\n",
    "        self.model_path = Path(\"model.onnx\")\n",
    "        self.settings_path = Path(\"settings.json\")\n",
    "        self.compiled_path = Path(\"model.ezkl\")\n",
    "        self.pk_path = Path(\"proving.key\")\n",
    "        self.vk_path = Path(\"verification.key\")\n",
    "        self.proof_path = Path(\"proof.json\")\n",
    "        self.input_path = Path(\"input.json\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.model = torch.hub.load(\n",
    "                repo_or_dir='snakers4/silero-vad', \n",
    "                model='silero_vad',\n",
    "                force_reload=True,\n",
    "                onnx=True\n",
    "            )\n",
    "            \n",
    "            # Export to ONNX if not exists\n",
    "            if not self.model_path.exists():\n",
    "                self.export_to_onnx()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"Model loading failed:\", e)\n",
    "            return False\n",
    "\n",
    "    def export_to_onnx(self):\n",
    "        dummy_input = torch.randn(1, self.window_size)\n",
    "        input_names = [\"input_values\"]\n",
    "        output_names = [\"last_hidden_state\"]\n",
    "        \n",
    "        torch.onnx.export(\n",
    "            self.model,\n",
    "            dummy_input,\n",
    "            self.model_path,\n",
    "            input_names=input_names,\n",
    "            output_names=output_names,\n",
    "            dynamic_axes={'input_values': {0: 'batch_size'}}\n",
    "        )\n",
    "\n",
    "    def preprocess_audio(self, audio_path):\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        return waveform.squeeze()\n",
    "\n",
    "    def setup_circuit(self):\n",
    "        if not self.model_path.exists():\n",
    "            if not self.load_model():\n",
    "                raise RuntimeError(\"Failed to load model\")\n",
    "        \n",
    "        # # Generate settings\n",
    "        if not self.settings_path.exists():\n",
    "            ezkl.gen_settings(self.model_path, self.settings_path)\n",
    "\n",
    "            # Calibrate settings\n",
    "            ezkl.calibrate_settings(self.model_path, self.settings_path, target=\"resources\")\n",
    "\n",
    "        if not self.compiled_path.exists():\n",
    "            # Compile circuit\n",
    "            ezkl.compile_circuit(self.model_path, self.compiled_path, self.settings_path)\n",
    "        \n",
    "        # Generate proving/verification keys\n",
    "        if not self.pk_path.exists() or not self.vk_path.exists():\n",
    "            ezkl.setup(\n",
    "                self.compiled_path,\n",
    "                self.pk_path,\n",
    "                self.vk_path,\n",
    "                self.settings_path\n",
    "            )\n",
    "\n",
    "    def create_proof(self, audio_path):\n",
    "        # Preprocess audio\n",
    "        waveform = self.preprocess_audio(audio_path)\n",
    "        inputs = self.processor(waveform, sampling_rate=self.sample_rate, return_tensors=\"pt\")\n",
    "        input_values = inputs.input_values.numpy().tolist()\n",
    "        \n",
    "        # Create input file\n",
    "        ezkl.create_input_file(input_values, self.input_path)\n",
    "        \n",
    "        # Generate proof\n",
    "        ezkl.prove(\n",
    "            self.model_path,\n",
    "            self.pk_path,\n",
    "            self.proof_path,\n",
    "            self.settings_path,\n",
    "            self.input_path\n",
    "        )\n",
    "        \n",
    "        return self.proof_path.exists()\n",
    "\n",
    "    def verify_proof(self):\n",
    "        return ezkl.verify(\n",
    "            self.proof_path,\n",
    "            self.vk_path,\n",
    "            self.settings_path\n",
    "        )\n",
    "\n",
    "    def run_workflow(self, audio_path):\n",
    "        self.setup_circuit()\n",
    "        proof_generated = self.create_proof(audio_path)\n",
    "        verification_status = self.verify_proof()\n",
    "        \n",
    "        return {\n",
    "            \"proof_generated\": proof_generated,\n",
    "            \"verification_status\": verification_status\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to /home/codespace/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading failed: 'tuple' object has no attribute 'modules'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m SileroEZKL()\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 113\u001b[0m, in \u001b[0;36mSileroEZKL.run_workflow\u001b[0;34m(self, audio_path)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_workflow\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio_path):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     proof_generated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proof(audio_path)\n\u001b[1;32m    115\u001b[0m     verification_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_proof()\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m, in \u001b[0;36mSileroEZKL.setup_circuit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model():\n\u001b[0;32m---> 63\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# # Generate settings\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings_path\u001b[38;5;241m.\u001b[39mexists():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load model"
     ]
    }
   ],
   "source": [
    "pipeline = SileroEZKL()\n",
    "results = pipeline.run_workflow(\"test.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
