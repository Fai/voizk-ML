{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c32aa0-9dbc-4a5d-9b1d-6dac92b3fefe",
   "metadata": {},
   "source": [
    "## RawNet3 with ezkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c1be92-c23a-4964-a47c-e311030b7c37",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate the integration of the RawNet3 model with ezkl, a toolkit for zero-knowledge proof systems. The goal is to run the RawNet3 model with ezkl and generate proofs for speaker verification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e292f-0ffa-461f-ab0d-36b3383528f8",
   "metadata": {},
   "source": [
    "#### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c72ec5ac-a3ba-4be8-afd1-536968dc41e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import ezkl\n",
    "import librosa\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from asteroid_filterbanks import Encoder, ParamSincFB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019bf33-9021-4c56-b25e-85a2cc75a877",
   "metadata": {},
   "source": [
    "#### RawNetBasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03e4b556-b597-4d7c-a20d-3c1ce71e07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreEmphasis(torch.nn.Module):\n",
    "    def __init__(self, coef: float = 0.97) -> None:\n",
    "        super().__init__()\n",
    "        self.coef = coef\n",
    "        self.register_buffer(\n",
    "            \"flipped_filter\",\n",
    "            torch.FloatTensor([-self.coef, 1.0]).unsqueeze(0).unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.tensor) -> torch.tensor:\n",
    "        assert (\n",
    "            len(input.size()) == 2\n",
    "        ), \"The number of dimensions of input tensor must be 2!\"\n",
    "        input = input.unsqueeze(1)\n",
    "        input = F.pad(input, (1, 0), \"reflect\")\n",
    "        return F.conv1d(input, self.flipped_filter)\n",
    "\n",
    "class AFMS(nn.Module):\n",
    "    def __init__(self, nb_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones((nb_dim, 1)))\n",
    "        self.fc = nn.Linear(nb_dim, nb_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.adaptive_avg_pool1d(x, 1).view(x.size(0), -1)\n",
    "        y = self.sig(self.fc(y)).view(x.size(0), x.size(1), -1)\n",
    "\n",
    "        x = x + self.alpha\n",
    "        x = x * y\n",
    "        return x\n",
    "\n",
    "class Bottle2neck(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes,\n",
    "        planes,\n",
    "        kernel_size=None,\n",
    "        dilation=None,\n",
    "        scale=4,\n",
    "        pool=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        width = int(math.floor(planes / scale))\n",
    "        self.conv1 = nn.Conv1d(inplanes, width * scale, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(width * scale)\n",
    "        self.nums = scale - 1\n",
    "        convs = []\n",
    "        bns = []\n",
    "        num_pad = math.floor(kernel_size / 2) * dilation\n",
    "        for i in range(self.nums):\n",
    "            convs.append(\n",
    "                nn.Conv1d(\n",
    "                    width,\n",
    "                    width,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                    padding=num_pad,\n",
    "                )\n",
    "            )\n",
    "            bns.append(nn.BatchNorm1d(width))\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.bns = nn.ModuleList(bns)\n",
    "        self.conv3 = nn.Conv1d(width * scale, planes, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.width = width\n",
    "        self.mp = nn.MaxPool1d(pool) if pool else False\n",
    "        self.afms = AFMS(planes)\n",
    "        if inplanes != planes:\n",
    "            self.residual = nn.Sequential(\n",
    "                nn.Conv1d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n",
    "            )\n",
    "        else:\n",
    "            self.residual = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn1(out)\n",
    "        spx = torch.split(out, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i == 0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.relu(sp)\n",
    "            sp = self.bns[i](sp)\n",
    "            if i == 0:\n",
    "                out = sp\n",
    "            else:\n",
    "                out = torch.cat((out, sp), 1)\n",
    "        out = torch.cat((out, spx[self.nums]), 1)\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn3(out)\n",
    "        out += residual\n",
    "        if self.mp:\n",
    "            out = self.mp(out)\n",
    "        out = self.afms(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716e587-5758-487d-a8b2-2caf36724f40",
   "metadata": {},
   "source": [
    "#### RawNet3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4d22f70-fd48-4ac0-9ebb-bb25b430cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RawNet3.py\n",
    "class RawNet3(nn.Module):\n",
    "    def __init__(self, block, model_scale, context, summed, C=1024, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        nOut = kwargs[\"nOut\"]\n",
    "\n",
    "        self.context = context\n",
    "        self.encoder_type = kwargs[\"encoder_type\"]\n",
    "        self.log_sinc = kwargs[\"log_sinc\"]\n",
    "        self.norm_sinc = kwargs[\"norm_sinc\"]\n",
    "        self.out_bn = kwargs[\"out_bn\"]\n",
    "        self.summed = summed\n",
    "\n",
    "        self.preprocess = nn.Sequential(\n",
    "            PreEmphasis(), nn.InstanceNorm1d(1, eps=1e-4, affine=True)\n",
    "        )\n",
    "        self.conv1 = Encoder(\n",
    "            ParamSincFB(\n",
    "                C // 4,\n",
    "                251,\n",
    "                stride=kwargs[\"sinc_stride\"],\n",
    "            )\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(C // 4)\n",
    "\n",
    "        self.layer1 = block(\n",
    "            C // 4, C, kernel_size=3, dilation=2, scale=model_scale, pool=5\n",
    "        )\n",
    "        self.layer2 = block(\n",
    "            C, C, kernel_size=3, dilation=3, scale=model_scale, pool=3\n",
    "        )\n",
    "        self.layer3 = block(C, C, kernel_size=3, dilation=4, scale=model_scale)\n",
    "        self.layer4 = nn.Conv1d(3 * C, 1536, kernel_size=1)\n",
    "\n",
    "        if self.context:\n",
    "            attn_input = 1536 * 3\n",
    "        else:\n",
    "            attn_input = 1536\n",
    "        print(\"self.encoder_type\", self.encoder_type)\n",
    "        if self.encoder_type == \"ECA\":\n",
    "            attn_output = 1536\n",
    "        elif self.encoder_type == \"ASP\":\n",
    "            attn_output = 1\n",
    "        else:\n",
    "            raise ValueError(\"Undefined encoder\")\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(attn_input, 128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Conv1d(128, attn_output, kernel_size=1),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "\n",
    "        self.bn5 = nn.BatchNorm1d(3072)\n",
    "\n",
    "        self.fc6 = nn.Linear(3072, nOut)\n",
    "        self.bn6 = nn.BatchNorm1d(nOut)\n",
    "\n",
    "        self.mp3 = nn.MaxPool1d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: input mini-batch (bs, samp)\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            x = self.preprocess(x)\n",
    "            x = torch.abs(self.conv1(x))\n",
    "            if self.log_sinc:\n",
    "                x = torch.log(x + 1e-6)\n",
    "            if self.norm_sinc == \"mean\":\n",
    "                x = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "            elif self.norm_sinc == \"mean_std\":\n",
    "                m = torch.mean(x, dim=-1, keepdim=True)\n",
    "                s = torch.std(x, dim=-1, keepdim=True)\n",
    "                s[s < 0.001] = 0.001\n",
    "                x = (x - m) / s\n",
    "\n",
    "        if self.summed:\n",
    "            x1 = self.layer1(x)\n",
    "            x2 = self.layer2(x1)\n",
    "            x3 = self.layer3(self.mp3(x1) + x2)\n",
    "        else:\n",
    "            x1 = self.layer1(x)\n",
    "            x2 = self.layer2(x1)\n",
    "            x3 = self.layer3(x2)\n",
    "\n",
    "        x = self.layer4(torch.cat((self.mp3(x1), x2, x3), dim=1))\n",
    "        x = self.relu(x)\n",
    "\n",
    "        t = x.size()[-1]\n",
    "\n",
    "        if self.context:\n",
    "            global_x = torch.cat(\n",
    "                (\n",
    "                    x,\n",
    "                    torch.mean(x, dim=2, keepdim=True).repeat(1, 1, t),\n",
    "                    torch.sqrt(\n",
    "                        torch.var(x, dim=2, keepdim=True).clamp(\n",
    "                            min=1e-4, max=1e4\n",
    "                        )\n",
    "                    ).repeat(1, 1, t),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            global_x = x\n",
    "\n",
    "        w = self.attention(global_x)\n",
    "\n",
    "        mu = torch.sum(x * w, dim=2)\n",
    "        sg = torch.sqrt(\n",
    "            (torch.sum((x**2) * w, dim=2) - mu**2).clamp(min=1e-4, max=1e4)\n",
    "        )\n",
    "\n",
    "        x = torch.cat((mu, sg), 1)\n",
    "\n",
    "        x = self.bn5(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        if self.out_bn:\n",
    "            x = self.bn6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47ec4f62-36f5-4b41-8e06-5d72839e9418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MainModel(**kwargs):\n",
    "\n",
    "    model = RawNet3(\n",
    "        Bottle2neck, model_scale=8, context=True, summed=True, **kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f793f3a1-14a9-4b3a-ae54-2b371593d110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.encoder_type ECA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/fbj1ksls7wq3z6dyxsdrrz9w0000gn/T/ipykernel_3210/1043732474.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./models/model.pt\", map_location=lambda storage, loc: storage)[\"model\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RawNet3(\n",
       "  (preprocess): Sequential(\n",
       "    (0): PreEmphasis()\n",
       "    (1): InstanceNorm1d(1, eps=0.0001, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (conv1): Encoder(\n",
       "    (filterbank): ParamSincFB()\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Bottle2neck(\n",
       "    (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convs): ModuleList(\n",
       "      (0-6): 7 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "    )\n",
       "    (bns): ModuleList(\n",
       "      (0-6): 7 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (mp): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (afms): AFMS(\n",
       "      (fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (sig): Sigmoid()\n",
       "    )\n",
       "    (residual): Sequential(\n",
       "      (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Bottle2neck(\n",
       "    (conv1): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convs): ModuleList(\n",
       "      (0-6): 7 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "    )\n",
       "    (bns): ModuleList(\n",
       "      (0-6): 7 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    (afms): AFMS(\n",
       "      (fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (sig): Sigmoid()\n",
       "    )\n",
       "    (residual): Identity()\n",
       "  )\n",
       "  (layer3): Bottle2neck(\n",
       "    (conv1): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convs): ModuleList(\n",
       "      (0-6): 7 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "    )\n",
       "    (bns): ModuleList(\n",
       "      (0-6): 7 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (afms): AFMS(\n",
       "      (fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (sig): Sigmoid()\n",
       "    )\n",
       "    (residual): Identity()\n",
       "  )\n",
       "  (layer4): Conv1d(3072, 1536, kernel_size=(1,), stride=(1,))\n",
       "  (attention): Sequential(\n",
       "    (0): Conv1d(4608, 128, kernel_size=(1,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv1d(128, 1536, kernel_size=(1,), stride=(1,))\n",
       "    (4): Softmax(dim=2)\n",
       "  )\n",
       "  (bn5): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc6): Linear(in_features=3072, out_features=256, bias=True)\n",
       "  (bn6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (mp3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained RawNet3 model\n",
    "model = MainModel(nOut=256, encoder_type=\"ECA\", log_sinc=True, norm_sinc=\"mean\", out_bn=False, sinc_stride=10)\n",
    "model.load_state_dict(torch.load(\"./models/model.pt\", map_location=lambda storage, loc: storage)[\"model\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa77e7-6c31-4eb0-bb8b-8cfae98e13ed",
   "metadata": {},
   "source": [
    "#### Load the pre-trained RawNet3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611185b-4430-4bbb-a751-bdee86ea53db",
   "metadata": {},
   "source": [
    "#### Specifying Files and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e26f78e-c85e-4f36-9b15-97abf74f939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify all the files we need\n",
    "model_path = os.path.join('rawnet3.onnx')\n",
    "data_path = os.path.join('input.json')\n",
    "cal_data_path = os.path.join('calibration.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c00af-5eaa-4378-a778-7cb388b9d412",
   "metadata": {},
   "source": [
    "#### Preparing Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f098038-2cc5-4ab1-8f3e-7ab7a071e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an audio file for testing\n",
    "audio_file = \"./sample1.wav\"\n",
    "audio, sample_rate = librosa.load(audio_file, sr=16000, mono=True)\n",
    "audio = audio[:48000]  # Truncate to 3 seconds (48000 samples)\n",
    "audio_tensor = torch.from_numpy(audio).float().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417b2c5-3214-4270-b6b8-43000672157a",
   "metadata": {},
   "source": [
    "## Integration with ezkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6184b66-a135-4f7e-a7b5-a2eaf208290e",
   "metadata": {},
   "source": [
    "#### Exporting the Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa0ee2bb-e110-457f-97f9-f3b22c206894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/fbj1ksls7wq3z6dyxsdrrz9w0000gn/T/ipykernel_3210/1709252647.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    }
   ],
   "source": [
    "# Export the model to ONNX\n",
    "torch.onnx.export(model,                     # model being run\n",
    "                  audio_tensor,              # model input\n",
    "                  'network.onnx',            # where to save the model\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91f5e987-e367-4686-b44c-82f9c266afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "data_array = audio_tensor.flatten().tolist()\n",
    "data = dict(input_data = [data_array])\n",
    "json.dump(data, open(data_path, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72736d29-8c4d-4e66-82cf-d4ce4b818bfd",
   "metadata": {},
   "source": [
    "#### Generating Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7b087e4-a5e7-40f5-bc57-aa2cc0f70b2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to generate settings: [graph] [tract] Translating node #71 \"If_109\" If ToTypedTranslator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mezkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to generate settings: [graph] [tract] Translating node #71 \"If_109\" If ToTypedTranslator"
     ]
    }
   ],
   "source": [
    "res = ezkl.gen_settings()\n",
    "assert res == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62430039-1cc2-4343-bc42-0be34ca0ae24",
   "metadata": {},
   "source": [
    "#### Error Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6857d-b068-4206-85a0-cf0ad63a6d92",
   "metadata": {},
   "source": [
    "This error indicates that there is an issue with generating the ezkl settings due to a specific node in the exported ONNX model. The error suggests that the exported ONNX model contains an \"If\" node (conditional node) that is not compatible with ezkl's type translator.\n",
    "\n",
    "\"If\" nodes are control flow structures that allow conditional execution of subgraphs based on a boolean condition. ezkl has limitations in handling complex operations like conditional statements in the model.\n",
    "\n",
    "## Limitations and Considerations\n",
    "\n",
    "When attempting to run a complex model like RawNet3 with ezkl, it's important to consider the limitations of ezkl in handling certain operations. ezkl is designed for zero-knowledge proofs and may not support all the complex operations present in the RawNet3 model. \n",
    "\n",
    "Some possible limitations and considerations include:\n",
    "\n",
    "1. **Conditional Statements**: ezkl may have difficulties in handling conditional statements like \"If\" nodes in the exported ONNX model. These nodes allow for conditional execution of subgraphs based on a boolean condition, which can be challenging to translate into a format compatible with ezkl's type translator.\n",
    "\n",
    "2. **Complex Architectures**: RawNet3 has a complex architecture with various layers and operations. Some of these operations may not be directly supported by ezkl, leading to compatibility issues during the translation process.\n",
    "\n",
    "3. **ONNX Export Configuration**: The configuration used while exporting the model to ONNX format can also impact compatibility with ezkl. Experimenting with different export configurations, such as changing the opset version or adjusting other export options, may\n",
    "\n",
    "5. **ONNX Model Preprocessing**: If the exported ONNX model contains problematic nodes like \"If\" nodes, preprocessing the model using tools like ONNX Simplifier or ONNX Optimizer may help in simplifying or removing these nodes before passing the model to ezkl.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we attempted to integrate the RawNet3 model with ezkl for speaker verification using zero-knowledge proofs. However, we encountered an error related to the compatibility of the exported ONNX model with ezkl's type translator.\n",
    "\n",
    "The error suggests that the presence of an \"If\" node in the exported model is causing issues during the generation of ezkl settings. This highlights the limitations of ezkl in handling complex operations like conditional statements.\n",
    "\n",
    "To overcome these limitations, we may need to simplify the RawNet3 model architecture, experiment with different ONNX export configurations, or preprocess the exported ONNX model to remove problematic nodes.\n",
    "\n",
    "It's important to note that running complex models like RawNet3 with ezkl may require significant modifications to the model architecture and careful consideration of the supported operations to ensure compatibility.\n",
    "\n",
    "Further investigation and experimentation would be necessary to successfully integrate RawNet3 with ezkl for speaker verification using zero-knowledge proofs. \n",
    "\n",
    "Despite the encountered challenges, the integration of RawNet3 with ezkl remains an interesting avenue for future research and development in the field of privacy-preserving speaker verification.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To further explore the integration of RawNet3 with ezkl, the following steps can be considered:\n",
    "\n",
    "1. **Model Simplification**: Analyze the RawNet3 model architecture and identify parts that use unsupported operations like conditional statements. Modify the model's source code to simplify or remove these parts while preserving the core functionality.\n",
    "\n",
    "2. **ONNX Export Optimization**: Experiment with different ONNX export configurations, such as changing the opset version or adjusting other export options, to find a configuration that generates an ONNX model compatible with ezkl.\n",
    "\n",
    "3. **ONNX Model Preprocessing**: Explore tools like ONNX Simplifier or ONNX Optimizer to preprocess the exported ONNX model and remove problematic nodes before passing it to ezkl.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85f12a-19e3-464c-8249-49481ceac2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
