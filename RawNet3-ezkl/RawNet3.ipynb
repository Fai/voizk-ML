{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d284b2-0f97-4ce0-a410-936cddd7061f",
   "metadata": {},
   "source": [
    "### RawNetBasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "630b1ff5-1d42-48a9-87e3-9b59e2d3093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from asteroid_filterbanks import Encoder, ParamSincFB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30320a5a-6dd5-4e4b-808d-12f4bd530948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PreEmphasis(torch.nn.Module):\n",
    "    def __init__(self, coef: float = 0.97) -> None:\n",
    "        super().__init__()\n",
    "        self.coef = coef\n",
    "        self.register_buffer(\n",
    "            \"flipped_filter\",\n",
    "            torch.FloatTensor([-self.coef, 1.0]).unsqueeze(0).unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.tensor) -> torch.tensor:\n",
    "        assert (\n",
    "            len(input.size()) == 2\n",
    "        ), \"The number of dimensions of input tensor must be 2!\"\n",
    "        input = input.unsqueeze(1)\n",
    "        input = F.pad(input, (1, 0), \"reflect\")\n",
    "        return F.conv1d(input, self.flipped_filter)\n",
    "\n",
    "class AFMS(nn.Module):\n",
    "    def __init__(self, nb_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones((nb_dim, 1)))\n",
    "        self.fc = nn.Linear(nb_dim, nb_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.adaptive_avg_pool1d(x, 1).view(x.size(0), -1)\n",
    "        y = self.sig(self.fc(y)).view(x.size(0), x.size(1), -1)\n",
    "\n",
    "        x = x + self.alpha\n",
    "        x = x * y\n",
    "        return x\n",
    "\n",
    "class Bottle2neck(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes,\n",
    "        planes,\n",
    "        kernel_size=None,\n",
    "        dilation=None,\n",
    "        scale=4,\n",
    "        pool=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        width = int(math.floor(planes / scale))\n",
    "        self.conv1 = nn.Conv1d(inplanes, width * scale, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(width * scale)\n",
    "        self.nums = scale - 1\n",
    "        convs = []\n",
    "        bns = []\n",
    "        num_pad = math.floor(kernel_size / 2) * dilation\n",
    "        for i in range(self.nums):\n",
    "            convs.append(\n",
    "                nn.Conv1d(\n",
    "                    width,\n",
    "                    width,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                    padding=num_pad,\n",
    "                )\n",
    "            )\n",
    "            bns.append(nn.BatchNorm1d(width))\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.bns = nn.ModuleList(bns)\n",
    "        self.conv3 = nn.Conv1d(width * scale, planes, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.width = width\n",
    "        self.mp = nn.MaxPool1d(pool) if pool else False\n",
    "        self.afms = AFMS(planes)\n",
    "        if inplanes != planes:\n",
    "            self.residual = nn.Sequential(\n",
    "                nn.Conv1d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n",
    "            )\n",
    "        else:\n",
    "            self.residual = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn1(out)\n",
    "        spx = torch.split(out, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i == 0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.relu(sp)\n",
    "            sp = self.bns[i](sp)\n",
    "            if i == 0:\n",
    "                out = sp\n",
    "            else:\n",
    "                out = torch.cat((out, sp), 1)\n",
    "        out = torch.cat((out, spx[self.nums]), 1)\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn3(out)\n",
    "        out += residual\n",
    "        if self.mp:\n",
    "            out = self.mp(out)\n",
    "        out = self.afms(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1551fde-835c-42a3-b1df-80bcb8691456",
   "metadata": {},
   "source": [
    "### RawNet3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b94c924-290a-4e90-9c36-826a994e995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RawNet3.py\n",
    "class RawNet3(nn.Module):\n",
    "    def __init__(self, block, model_scale, context, summed, C=1024, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        nOut = kwargs[\"nOut\"]\n",
    "\n",
    "        self.context = context\n",
    "        self.encoder_type = kwargs[\"encoder_type\"]\n",
    "        self.log_sinc = kwargs[\"log_sinc\"]\n",
    "        self.norm_sinc = kwargs[\"norm_sinc\"]\n",
    "        self.out_bn = kwargs[\"out_bn\"]\n",
    "        self.summed = summed\n",
    "\n",
    "        self.preprocess = nn.Sequential(\n",
    "            PreEmphasis(), nn.InstanceNorm1d(1, eps=1e-4, affine=True)\n",
    "        )\n",
    "        self.conv1 = Encoder(\n",
    "            ParamSincFB(\n",
    "                C // 4,\n",
    "                251,\n",
    "                stride=kwargs[\"sinc_stride\"],\n",
    "            )\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(C // 4)\n",
    "\n",
    "        self.layer1 = block(\n",
    "            C // 4, C, kernel_size=3, dilation=2, scale=model_scale, pool=5\n",
    "        )\n",
    "        self.layer2 = block(\n",
    "            C, C, kernel_size=3, dilation=3, scale=model_scale, pool=3\n",
    "        )\n",
    "        self.layer3 = block(C, C, kernel_size=3, dilation=4, scale=model_scale)\n",
    "        self.layer4 = nn.Conv1d(3 * C, 1536, kernel_size=1)\n",
    "\n",
    "        if self.context:\n",
    "            attn_input = 1536 * 3\n",
    "        else:\n",
    "            attn_input = 1536\n",
    "        print(\"self.encoder_type\", self.encoder_type)\n",
    "        if self.encoder_type == \"ECA\":\n",
    "            attn_output = 1536\n",
    "        elif self.encoder_type == \"ASP\":\n",
    "            attn_output = 1\n",
    "        else:\n",
    "            raise ValueError(\"Undefined encoder\")\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(attn_input, 128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Conv1d(128, attn_output, kernel_size=1),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "\n",
    "        self.bn5 = nn.BatchNorm1d(3072)\n",
    "\n",
    "        self.fc6 = nn.Linear(3072, nOut)\n",
    "        self.bn6 = nn.BatchNorm1d(nOut)\n",
    "\n",
    "        self.mp3 = nn.MaxPool1d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: input mini-batch (bs, samp)\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            x = self.preprocess(x)\n",
    "            x = torch.abs(self.conv1(x))\n",
    "            if self.log_sinc:\n",
    "                x = torch.log(x + 1e-6)\n",
    "            if self.norm_sinc == \"mean\":\n",
    "                x = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "            elif self.norm_sinc == \"mean_std\":\n",
    "                m = torch.mean(x, dim=-1, keepdim=True)\n",
    "                s = torch.std(x, dim=-1, keepdim=True)\n",
    "                s[s < 0.001] = 0.001\n",
    "                x = (x - m) / s\n",
    "\n",
    "        if self.summed:\n",
    "            x1 = self.layer1(x)\n",
    "            x2 = self.layer2(x1)\n",
    "            x3 = self.layer3(self.mp3(x1) + x2)\n",
    "        else:\n",
    "            x1 = self.layer1(x)\n",
    "            x2 = self.layer2(x1)\n",
    "            x3 = self.layer3(x2)\n",
    "\n",
    "        x = self.layer4(torch.cat((self.mp3(x1), x2, x3), dim=1))\n",
    "        x = self.relu(x)\n",
    "\n",
    "        t = x.size()[-1]\n",
    "\n",
    "        if self.context:\n",
    "            global_x = torch.cat(\n",
    "                (\n",
    "                    x,\n",
    "                    torch.mean(x, dim=2, keepdim=True).repeat(1, 1, t),\n",
    "                    torch.sqrt(\n",
    "                        torch.var(x, dim=2, keepdim=True).clamp(\n",
    "                            min=1e-4, max=1e4\n",
    "                        )\n",
    "                    ).repeat(1, 1, t),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            global_x = x\n",
    "\n",
    "        w = self.attention(global_x)\n",
    "\n",
    "        mu = torch.sum(x * w, dim=2)\n",
    "        sg = torch.sqrt(\n",
    "            (torch.sum((x**2) * w, dim=2) - mu**2).clamp(min=1e-4, max=1e4)\n",
    "        )\n",
    "\n",
    "        x = torch.cat((mu, sg), 1)\n",
    "\n",
    "        x = self.bn5(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        if self.out_bn:\n",
    "            x = self.bn6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37b3bf27-a440-4f47-86c5-4b43b17f4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MainModel(**kwargs):\n",
    "\n",
    "    model = RawNet3(\n",
    "        Bottle2neck, model_scale=8, context=True, summed=True, **kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d41b3e-d648-4b35-9c0a-df872e1a34d0",
   "metadata": {},
   "source": [
    "#### Load pre-trained weights from the submodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32197ad0-4d32-4305-9baa-4369c79fbe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.encoder_type ECA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/fbj1ksls7wq3z6dyxsdrrz9w0000gn/T/ipykernel_68909/3791338880.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./models/model.pt\", map_location=lambda storage, loc: storage)[\"model\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RawNet3(\n",
       "  (preprocess): Sequential(\n",
       "    (0): PreEmphasis()\n",
       "    (1): InstanceNorm1d(1, eps=0.0001, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (conv1): Encoder(\n",
       "    (filterbank): ParamSincFB()\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Bottle2neck(\n",
       "    (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convs): ModuleList(\n",
       "      (0-6): 7 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "    )\n",
       "    (bns): ModuleList(\n",
       "      (0-6): 7 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (mp): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (afms): AFMS(\n",
       "      (fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (sig): Sigmoid()\n",
       "    )\n",
       "    (residual): Sequential(\n",
       "      (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Bottle2neck(\n",
       "    (conv1): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convs): ModuleList(\n",
       "      (0-6): 7 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "    )\n",
       "    (bns): ModuleList(\n",
       "      (0-6): 7 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    (afms): AFMS(\n",
       "      (fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (sig): Sigmoid()\n",
       "    )\n",
       "    (residual): Identity()\n",
       "  )\n",
       "  (layer3): Bottle2neck(\n",
       "    (conv1): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convs): ModuleList(\n",
       "      (0-6): 7 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "    )\n",
       "    (bns): ModuleList(\n",
       "      (0-6): 7 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (afms): AFMS(\n",
       "      (fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (sig): Sigmoid()\n",
       "    )\n",
       "    (residual): Identity()\n",
       "  )\n",
       "  (layer4): Conv1d(3072, 1536, kernel_size=(1,), stride=(1,))\n",
       "  (attention): Sequential(\n",
       "    (0): Conv1d(4608, 128, kernel_size=(1,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv1d(128, 1536, kernel_size=(1,), stride=(1,))\n",
       "    (4): Softmax(dim=2)\n",
       "  )\n",
       "  (bn5): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc6): Linear(in_features=3072, out_features=256, bias=True)\n",
       "  (bn6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (mp3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MainModel(nOut=256, encoder_type=\"ECA\", log_sinc=True, norm_sinc=\"mean\", out_bn=False, sinc_stride=10)\n",
    "model.load_state_dict(torch.load(\"./models/model.pt\", map_location=lambda storage, loc: storage)[\"model\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3677d5-6db9-4160-b53b-fc983eee5414",
   "metadata": {},
   "source": [
    "#### Extract embeddings of audio file\n",
    "\n",
    "This function takes an audio file, splits it into segments, extracts the embeddings using the RawNet3 model, and returns the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303b368-077e-410e-8d00-360eb866dd83",
   "metadata": {},
   "source": [
    "An embedding in the context of RawNet3 is a compact and discriminative vector representation of an audio segment that captures the unique characteristics of the speaker, allowing for various speech processing and analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a7aa925-531f-4bf7-a2ac-0a3fe2cf9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def extract_speaker_embd(model, audio_file, n_samples=48000, n_segments=10, gpu=False):\n",
    "    audio, sample_rate = librosa.load(audio_file, sr=16000, mono=True)\n",
    "    \n",
    "    if len(audio) < n_samples:\n",
    "        shortage = n_samples - len(audio) + 1\n",
    "        audio = np.pad(audio, (0, shortage), \"wrap\")\n",
    "    \n",
    "    audios = []\n",
    "    startframe = np.linspace(0, len(audio) - n_samples, num=n_segments)\n",
    "    for asf in startframe:\n",
    "        audios.append(audio[int(asf):int(asf) + n_samples])\n",
    "    \n",
    "    audios = torch.from_numpy(np.stack(audios, axis=0).astype(np.float32))\n",
    "    if gpu:\n",
    "        audios = audios.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output = model(audios)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88cf4eee-c25c-41c7-8cb9-bf1b228df335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/fbj1ksls7wq3z6dyxsdrrz9w0000gn/T/ipykernel_68909/1709252647.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "audio_file = \"./sample1.wav\"\n",
    "embeddings = extract_speaker_embd(model, audio_file, n_samples=48000, n_segments=10)\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9edbe39c-6b06-46eb-8d94-184cd833e347",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not enough disk space. Needed: 107.98 GiB (download: 4.68 MiB, generated: 107.98 GiB)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Cargar el conjunto de datos VoxCeleb\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvoxceleb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain[:10\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Función para extraer embeddings de un archivo de audio\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_speaker_embd\u001b[39m(model, audio, n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48000\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Preprocesar el audio\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:176\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:661\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n\u001b[1;32m    655\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m _fetch_builder(\n\u001b[1;32m    656\u001b[0m     name,\n\u001b[1;32m    657\u001b[0m     data_dir,\n\u001b[1;32m    658\u001b[0m     builder_kwargs,\n\u001b[1;32m    659\u001b[0m     try_gcs,\n\u001b[1;32m    660\u001b[0m )\n\u001b[0;32m--> 661\u001b[0m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    664\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:517\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[0;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m    516\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 517\u001b[0m   \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:176\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:702\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format, permissions)\u001b[0m\n\u001b[1;32m    697\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating dataset \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mhas_sufficient_disk_space(\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size,\n\u001b[1;32m    700\u001b[0m     directory\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mfspath(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir_root),\n\u001b[1;32m    701\u001b[0m ):\n\u001b[0;32m--> 702\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    703\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough disk space. Needed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (download: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, generated: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    704\u001b[0m       \u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    705\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size,\n\u001b[1;32m    706\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size,\n\u001b[1;32m    707\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size,\n\u001b[1;32m    708\u001b[0m       )\n\u001b[1;32m    709\u001b[0m   )\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_download_bytes()\n\u001b[1;32m    712\u001b[0m dl_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_download_manager(\n\u001b[1;32m    713\u001b[0m     download_dir\u001b[38;5;241m=\u001b[39mdownload_dir,\n\u001b[1;32m    714\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m    715\u001b[0m )\n",
      "\u001b[0;31mOSError\u001b[0m: Not enough disk space. Needed: 107.98 GiB (download: 4.68 MiB, generated: 107.98 GiB)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el conjunto de datos VoxCeleb\n",
    "dataset = tfds.load('voxceleb', split='train[:10%]', shuffle_files=True, as_supervised=True)\n",
    "\n",
    "# Función para extraer embeddings de un archivo de audio\n",
    "def extract_speaker_embd(model, audio, n_samples=48000):\n",
    "    # Preprocesar el audio\n",
    "    audio = audio[:n_samples]\n",
    "    if len(audio) < n_samples:\n",
    "        audio = np.pad(audio, (0, n_samples - len(audio)), 'wrap')\n",
    "    \n",
    "    # Convertir el audio a un tensor de TensorFlow\n",
    "    audio_tensor = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "    audio_tensor = tf.expand_dims(audio_tensor, axis=0)\n",
    "    \n",
    "    # Obtener los embeddings utilizando el modelo\n",
    "    embeddings = model(audio_tensor)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Cargar los trials de Vox1-O\n",
    "with open(\"./trials/cleaned_test_list.txt\", \"r\") as f:\n",
    "    trials = f.readlines()\n",
    "\n",
    "# Extraer embeddings para cada archivo de audio único en los trials\n",
    "files = list(itertools.chain(*[x.strip().split()[-2:] for x in trials]))\n",
    "setfiles = list(set(files))\n",
    "setfiles.sort()\n",
    "print(\"setfiles: \", setfiles)\n",
    "\n",
    "embd_dic = {}\n",
    "missing_files = []\n",
    "\n",
    "for f in tqdm(setfiles):\n",
    "    try:\n",
    "        # Obtener el ejemplo de audio correspondiente al archivo\n",
    "        audio = next(iter(dataset.filter(lambda x: x[1].numpy().decode('utf-8') == f)))[0].numpy()\n",
    "        \n",
    "        # Extraer los embeddings del audio\n",
    "        embeddings = extract_speaker_embd(model, audio, n_samples=48000)\n",
    "        embd_dic[f] = embeddings\n",
    "    except StopIteration:\n",
    "        missing_files.append(f)\n",
    "        continue\n",
    "\n",
    "print(\"embd_dic: \", embd_dic)\n",
    "\n",
    "# Calcular puntuaciones de verificación para cada par de archivos en los trials\n",
    "labels, scores = [], []\n",
    "for line in trials:\n",
    "    data = line.split()\n",
    "    try:\n",
    "        ref_feat = tf.nn.l2_normalize(embd_dic[data[1]], axis=1)\n",
    "        com_feat = tf.nn.l2_normalize(embd_dic[data[2]], axis=1)\n",
    "    except KeyError:\n",
    "        missing_files.append(line.strip())\n",
    "        continue\n",
    "    \n",
    "    dist = tf.reduce_mean(tf.square(ref_feat - com_feat), axis=1)\n",
    "    score = -1.0 * tf.reduce_mean(dist).numpy()\n",
    "    labels.append(int(data[0]))\n",
    "    scores.append(score)\n",
    "\n",
    "# Imprimir el resumen de archivos faltantes\n",
    "if missing_files:\n",
    "    print(f\"Skipped {len(missing_files)} trials due to missing audio files.\")\n",
    "    print(\"Missing files:\")\n",
    "    for file in missing_files:\n",
    "        print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd15405-4226-4f40-8eac-44e0b054fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el conjunto de datos VoxCeleb\n",
    "dataset = tfds.load('voxceleb', split='train', shuffle_files=True, as_supervised=True, download_and_prepare_kwargs={'max_examples_per_split': 10000})\n",
    "\n",
    "# Función para extraer embeddings de un archivo de audio\n",
    "def extract_speaker_embd(model, audio, n_samples=48000):\n",
    "    # Preprocesar el audio\n",
    "    audio = audio[:n_samples]\n",
    "    if len(audio) < n_samples:\n",
    "        audio = np.pad(audio, (0, n_samples - len(audio)), 'wrap')\n",
    "    \n",
    "    # Convertir el audio a un tensor de TensorFlow\n",
    "    audio_tensor = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "    audio_tensor = tf.expand_dims(audio_tensor, axis=0)\n",
    "    \n",
    "    # Obtener los embeddings utilizando el modelo\n",
    "    embeddings = model(audio_tensor)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Cargar los trials de Vox1-O\n",
    "with open(\"./trials/cleaned_test_list.txt\", \"r\") as f:\n",
    "    trials = f.readlines()\n",
    "\n",
    "# Extraer embeddings para cada archivo de audio único en los trials\n",
    "files = list(itertools.chain(*[x.strip().split()[-2:] for x in trials]))\n",
    "setfiles = list(set(files))\n",
    "setfiles.sort()\n",
    "print(\"setfiles: \", setfiles)\n",
    "\n",
    "embd_dic = {}\n",
    "missing_files = []\n",
    "\n",
    "for f in tqdm(setfiles):\n",
    "    try:\n",
    "        # Obtener el ejemplo de audio correspondiente al archivo\n",
    "        audio = next(iter(dataset.filter(lambda x: x[1].numpy().decode('utf-8') == f)))[0].numpy()\n",
    "        \n",
    "        # Extraer los embeddings del audio\n",
    "        embeddings = extract_speaker_embd(model, audio, n_samples=48000)\n",
    "        embd_dic[f] = embeddings\n",
    "    except StopIteration:\n",
    "        missing_files.append(f)\n",
    "        continue\n",
    "\n",
    "print(\"embd_dic: \", embd_dic)\n",
    "\n",
    "# Calcular puntuaciones de verificación para cada par de archivos en los trials\n",
    "labels, scores = [], []\n",
    "for line in trials:\n",
    "    data = line.split()\n",
    "    try:\n",
    "        ref_feat = tf.nn.l2_normalize(embd_dic[data[1]], axis=1)\n",
    "        com_feat = tf.nn.l2_normalize(embd_dic[data[2]], axis=1)\n",
    "    except KeyError:\n",
    "        missing_files.append(line.strip())\n",
    "        continue\n",
    "    \n",
    "    dist = tf.reduce_mean(tf.square(ref_feat - com_feat), axis=1)\n",
    "    score = -1.0 * tf.reduce_mean(dist).numpy()\n",
    "    labels.append(int(data[0]))\n",
    "    scores.append(score)\n",
    "\n",
    "# Imprimir el resumen de archivos faltantes\n",
    "if missing_files:\n",
    "    print(f\"Skipped {len(missing_files)} trials due to missing audio files.\")\n",
    "    print(\"Missing files:\")\n",
    "    for file in missing_files:\n",
    "        print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f3976-fd56-476e-b884-f133723f30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el conjunto de datos VoxCeleb\n",
    "dataset = tfds.load('voxceleb', split='train')\n",
    "\n",
    "# Función para extraer embeddings de un archivo de audio\n",
    "def extract_speaker_embd(model, audio, n_samples=48000):\n",
    "    # Preprocesar el audio\n",
    "    audio = audio[:n_samples]\n",
    "    if len(audio) < n_samples:\n",
    "        audio = np.pad(audio, (0, n_samples - len(audio)), 'wrap')\n",
    "    \n",
    "    # Convertir el audio a un tensor de TensorFlow\n",
    "    audio_tensor = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "    audio_tensor = tf.expand_dims(audio_tensor, axis=0)\n",
    "    \n",
    "    # Obtener los embeddings utilizando el modelo\n",
    "    embeddings = model(audio_tensor)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Cargar los trials de Vox1-O\n",
    "with open(\"./trials/cleaned_test_list.txt\", \"r\") as f:\n",
    "    trials = f.readlines()\n",
    "\n",
    "# Extraer embeddings para cada archivo de audio único en los trials\n",
    "files = list(itertools.chain(*[x.strip().split()[-2:] for x in trials]))\n",
    "setfiles = list(set(files))\n",
    "setfiles.sort()\n",
    "print(\"setfiles: \", setfiles)\n",
    "\n",
    "embd_dic = {}\n",
    "missing_files = []\n",
    "\n",
    "for f in tqdm(setfiles):\n",
    "    try:\n",
    "        # Obtener el ejemplo de audio correspondiente al archivo\n",
    "        example = next(iter(dataset.filter(lambda x: x['youtube_id'] == f)))\n",
    "        audio = example['audio'].numpy()\n",
    "        \n",
    "        # Extraer los embeddings del audio\n",
    "        embeddings = extract_speaker_embd(model, audio, n_samples=48000)\n",
    "        embd_dic[f] = embeddings\n",
    "    except StopIteration:\n",
    "        missing_files.append(f)\n",
    "        continue\n",
    "\n",
    "print(\"embd_dic: \", embd_dic)\n",
    "\n",
    "# Calcular puntuaciones de verificación para cada par de archivos en los trials\n",
    "labels, scores = [], []\n",
    "for line in trials:\n",
    "    data = line.split()\n",
    "    try:\n",
    "        ref_feat = tf.nn.l2_normalize(embd_dic[data[1]], axis=1)\n",
    "        com_feat = tf.nn.l2_normalize(embd_dic[data[2]], axis=1)\n",
    "    except KeyError:\n",
    "        missing_files.append(line.strip())\n",
    "        continue\n",
    "    \n",
    "    dist = tf.reduce_mean(tf.square(ref_feat - com_feat), axis=1)\n",
    "    score = -1.0 * tf.reduce_mean(dist).numpy()\n",
    "    labels.append(int(data[0]))\n",
    "    scores.append(score)\n",
    "\n",
    "# Imprimir el resumen de archivos faltantes\n",
    "if missing_files:\n",
    "    print(f\"Skipped {len(missing_files)} trials due to missing audio files.\")\n",
    "    print(\"Missing files:\")\n",
    "    for file in missing_files:\n",
    "        print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79b475-4e9b-45a8-8ad5-97c7b39ad490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa96176-1f3d-4618-a393-394d31e5770b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637b48e-dd66-4cc9-af4f-c656d65f6475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
